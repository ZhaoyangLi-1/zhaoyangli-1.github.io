---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello! I am a Research Assistant at [USC](https://www.usc.edu/), fortunate to be advised by [Erdem Biyik](https://ebiyik.github.io/). Previously, I completed my Masterâ€™s in [Electrical and Computer Engineering](https://www.ece.ucsd.edu/) at [UC San Diego](https://www.ucsd.edu/), where I worked with [Hao Su](https://cseweb.ucsd.edu/~haosu/) and [Pengtao Xie](https://pengtaoxie.github.io/). I also earned a double major Bachelorâ€™s degree in [Computer Science](https://guide.wisc.edu/undergraduate/letters-science/computer-sciences/computer-sciences-bs/) and [Mathematics](https://guide.wisc.edu/undergraduate/letters-science/mathematics/mathematics-ba/mathematics-mathematics-data-science-ba/#text) from [UWâ€“Madison](https://www.wisc.edu/), where I worked with [Vikas Singh](https://www.biostat.wisc.edu/~vsingh/).

My research is about building embodied agents that can see, decide, and act in the real world. I work at the intersection of multimodal learning such as vision language models and robot leanring including imitation learning, reinforcement learning, and preference alignment. Iâ€™m especially interested in turning open-ended language goals into grounded sub-tasks, then training policies that are sample-efficient and robust enough to run on real applications. My goal is to create interpretable, safe robots that generalize beyond curated demos and handle the messy, everyday settings we live in.

ðŸš€ I am currently applying for PhD programs for Fall 2026. You can see my [CV](/files/Zhaoyang_Li_CV.pdf).

Publications and Preprints
======

\* indicates equal contribution.

<style>
#pubs .card, #pubs .panel, #pubs .list-group-item, #pubs .media,
#pubs .paper-card, #pubs .project-card, #pubs .article-card,
#pubs .archive__item, #pubs .archive__item-teaser {
  border: 0 !important;
  box-shadow: none !important;
  background: transparent !important;
}
#pubs table, #pubs tr, #pubs td {
  border: 0 !important;
  background: transparent !important;
}
#pubs .card, #pubs .panel, #pubs .list-group-item,
#pubs .archive__item { padding: 0 0 16px 0; margin: 0 0 18px 0; }
#pubs {
  font-size: 21px;      /* åŸºç¡€å­—ä½“ */
  line-height: 2.0;
}
</style>

<div id="pubs">
<table border="0" width="100%" cellspacing="12" cellpadding="0">
  <!-- ORIC -->
  <tr>
    <td width="300" valign="top">
      <img src="images/ORIC.png" width="300" alt="ORIC" loading="lazy">
    </td>
    <td valign="top">
      <b>Zhaoyang Li<sup>*</sup></b>, Zhan Ling<sup>*</sup>, Yuchen Zhou, Hao Su.<br>
      <b>ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models.</b><br>
      <i>International Conference on Computer Vision (ICCV) 2025 Workshop on MMRAgI</i>; extended version in submission.<br>
      <a href="https://arxiv.org/abs/2509.15695">Paper</a> Â·
      <a href="https://github.com/ZhaoyangLi-1/ORIC">Code</a>
    </td>
  </tr>

  <!-- InteractGPT -->
  <tr>
    <td width="300" valign="top">
      <img src="images/InteractGPT.png" width="300" alt="InteractGPT" loading="lazy">
    </td>
    <td valign="top">
      <b>Zhaoyang Li</b>, Sushaanth Srinivasan, Ninad Ekbote, Pengtao Xie.<br>
      <b>A Multi-modal Large Language Model for Predicting Mechanisms of Drug Interactions.</b><br>
      <i>Under Review, 2025.</i>
    </td>
  </tr>

  <!-- S2V-Dagger -->
  <tr>
    <td width="300" valign="top">
      <img src="images/s2v_dagger.png" width="300" alt="S2V-Dagger" loading="lazy">
    </td>
    <td valign="top">
      Tongzhou Mu<sup>*</sup>, <b>Zhaoyang Li<sup>*</sup></b>, StanisÅ‚aw Wiktor Strzelecki<sup>*</sup>, Xiu Yuan, Yunchao Yao, Litian Liang, Aditya Gulati, Hao Su.<br>
      <b>When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?</b><br>
      <i>AAAI Conference on Artificial Intelligence (AAAI) 2025.</i><br>
      <a href="https://arxiv.org/abs/2412.13662">Paper</a> Â·
      <a href="https://github.com/ZhaoyangLi-1/s2v-dagger">Code</a>
    </td>
  </tr>

  <!-- JST -->
  <tr>
    <td width="300" valign="top">
      <img src="images/jst.png" width="300" alt="JST" loading="lazy">
    </td>
    <td valign="top">
      Yifei Zhang, Yusen Jiao, Jiayi Chen, <b>Zhaoyang Li</b>, Huaxiu Yao, Jieyu Zhang, Frederic Sala.<br>
      <b>Just Select Twice: Leveraging Low-Quality Data to Improve Data Selection.</b><br>
      <i>ATTRIB Workshop at NeurIPS 2024; extended version in submission.</i><br>
      <a href="https://openreview.net/forum?id=dugoA2gfhs">Paper</a>
    </td>
  </tr>
</table>

</div>


Professional Service
======
- Reviewer, AAAI 2025 Workshop on Large Language Models and Generative AI for Health  
- Reviewer, AAAI 2026

Teaching 
======
Teaching Assistant at UWâ€“Madison â€” Spring 2023  
- CS540: Introduction to Artificial Intelligence  

Peer Mentor at UWâ€“Madison â€” Fall 2022  
- CS537: Introduction to Operating System  





