---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello! I am a Research Assistant at [USC](https://www.usc.edu/), fortunate to be advised by [Erdem Biyik](https://ebiyik.github.io/). Previously, I completed my Masterâ€™s in [Electrical and Computer Engineering](https://www.ece.ucsd.edu/) at [UC San Diego](https://www.ucsd.edu/), where I worked with [Hao Su](https://cseweb.ucsd.edu/~haosu/) and [Pengtao Xie](https://pengtaoxie.github.io/). I also earned a double major Bachelorâ€™s degree in [Computer Science](https://guide.wisc.edu/undergraduate/letters-science/computer-sciences/computer-sciences-bs/) and [Mathematics](https://guide.wisc.edu/undergraduate/letters-science/mathematics/mathematics-ba/mathematics-mathematics-data-science-ba/#text) from [UWâ€“Madison](https://www.wisc.edu/), where I worked with [Vikas Singh](https://www.biostat.wisc.edu/~vsingh/).


My research focuses on developing embodied agents that can perceive, reason, and act in the physical world. I work at the intersection of **multimodal learning**, **robot learning**, and **preference alignment**. Broadly, I am interested in:

- **Visionâ€“Language Models:** Enhancing the robustness of large visionâ€“language models, with a focus on improving their physical grounding in real-world environments and their applications.
- **Efficient Control and Planning:** Developing effective policies through imitation learning, reinforcement learning, and diffusion-based generative methods.
- **Visionâ€“Languageâ€“Action (VLA) Integration:** Integrating visionâ€“languageâ€“action systems into unified embodied agents 
- **Human-centered Alignment:** Grounding agentsâ€™ behaviors in human intentions using feedback, pairwise preferences, and other weak supervision.

Ultimately, my goal is to build interpretable, reliable, and safe embodied agents that generalize beyond curated demonstrations and operate effectively in open-world environments.


ðŸš€ I am currently applying for PhD programs for Fall 2026. You can see my [CV](/files/Zhaoyang_Li_CV.pdf).

Publications and Preprints
======

\* indicates equal contribution.

<style>
#pubs .card, #pubs .panel, #pubs .list-group-item, #pubs .media,
#pubs .paper-card, #pubs .project-card, #pubs .article-card,
#pubs .archive__item, #pubs .archive__item-teaser {
  border: 0 !important;
  box-shadow: none !important;
  background: transparent !important;
}
#pubs table, #pubs tr, #pubs td {
  border: 0 !important;
  background: transparent !important;
}
#pubs .card, #pubs .panel, #pubs .list-group-item,
#pubs .archive__item { padding: 0 0 16px 0; margin: 0 0 18px 0; }
#pubs {
  font-size: 21px;      /* åŸºç¡€å­—ä½“ */
  line-height: 2.0;
}
</style>

<div id="pubs">
<table border="0" width="100%" cellspacing="12" cellpadding="0">
  <!-- ORIC -->
  <tr>
    <td width="280" valign="top">
      <img src="images/ORIC.png" width="280" alt="ORIC" loading="lazy">
    </td>
    <td valign="top">
      <b>ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models.</b><br>
      <b>Zhaoyang Li<sup>*</sup></b>, Zhan Ling<sup>*</sup>, Yuchen Zhou, Litian Gong, Erdem BÄ±yÄ±k, Hao Su.<br>
      <i>MMRAg Workshop at International Conference on Computer Vision (ICCV) 2025</i>; extended version in submission.<br>
      <a href="https://arxiv.org/abs/2509.15695">Paper</a> Â·
      <a href="https://github.com/ZhaoyangLi-1/ORIC">Code</a>
    </td>
  </tr>

  <!-- InteractGPT -->
  <tr>
    <td width="280" valign="top">
      <img src="images/InteractGPT.png" width="280" alt="InteractGPT" loading="lazy">
    </td>
    <td valign="top">
      <b>A Multi-modal Large Language Model for Predicting Mechanisms of Drug Interactions.</b><br>
      <b>Zhaoyang Li</b>, Sushaanth Srinivasan, Ninad Ekbote, Pengtao Xie.<br>
      <i>Under Review, 2025.</i>
    </td>
  </tr>

  <!-- S2V-Dagger -->
  <tr>
    <td width="280" valign="top">
      <img src="images/s2v_dagger.png" width="280" alt="S2V-Dagger" loading="lazy">
    </td>
    <td valign="top">
      <b>When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?</b><br>
      Tongzhou Mu<sup>*</sup>, <b>Zhaoyang Li<sup>*</sup></b>, StanisÅ‚aw Wiktor Strzelecki<sup>*</sup>, Xiu Yuan, Yunchao Yao, Litian Liang, Aditya Gulati, Hao Su.<br>
      <i>AAAI Conference on Artificial Intelligence (AAAI) 2025.</i><br>
      <a href="https://arxiv.org/abs/2412.13662">Paper</a> Â·
      <a href="https://github.com/ZhaoyangLi-1/s2v-dagger">Code</a>
    </td>
  </tr>

  <!-- JST -->
  <tr>
    <td width="280" valign="top">
      <img src="images/jst.png" width="280" alt="JST" loading="lazy">
    </td>
    <td valign="top">
      <b>Just Select Twice: Leveraging Low-Quality Data to Improve Data Selection.</b><br>
      Yifei Zhang, Yusen Jiao, Jiayi Chen, <b>Zhaoyang Li</b>, Huaxiu Yao, Jieyu Zhang, Frederic Sala.<br>
      <i>ATTRIB Workshop at Conference on Neural Information Processing Systems (NeurIPS) 2024; extended version in submission.</i><br>
      <a href="https://openreview.net/forum?id=dugoA2gfhs">Paper</a>
    </td>
  </tr>
</table>

</div>

Educations
======
- **M.S. in Electrical and Computer Engineering (Intelligent Systems, Robotics & Control)**  
  University of California, San Diego | 09/2023 â€“ 06/2025

- **B.S. in Computer Science & Mathematics (Double Major)**  
  University of Wisconsinâ€“Madison | 01/2021 â€“ 05/2023



Professional Service
======
- Reviewer, AAAI 2025 Workshop on Large Language Models and Generative AI for Health  
- Reviewer, AAAI 2026

Teaching 
======
Teaching Assistant at UWâ€“Madison â€” Spring 2023  
- CS540: Introduction to Artificial Intelligence  

Peer Mentor at UWâ€“Madison â€” Fall 2022  
- CS537: Introduction to Operating System  





